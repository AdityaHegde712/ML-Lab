{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1. Use RAND to generate synthetic dataset\n",
    "2. Choose any ANN dataset kaggle or uci machine learning repository\n",
    "\n",
    "For both, \n",
    "1. Implement step-by-step for each with a standard set of weights etc\n",
    "2. Show tabular representation of hyperparameters as they are tuned at the end. Use CSVLogger code from earlier\n",
    "\n",
    "Note: <br>\n",
    "Decide activation functions as well, and define the neural network architecture <br>\n",
    "Define weights etc, create sub functions for forward and backward propagations <br>\n",
    "Set a number of iterations and run through the architectureÂ accordingly <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# def der_sigmoid(x):\n",
    "#     return x * (1 - x)\n",
    "\n",
    "\n",
    "# class my_ANN():\n",
    "#     def __init__(self, learning_rate=0.001, n_hidden_layers=3, weights=None, random_state=42, output_dim=1, neurons_per_layer=[10, 10, 1]):\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.n_hidden_layers = n_hidden_layers\n",
    "#         self.weights = weights\n",
    "#         self.neurons_per_layer = neurons_per_layer\n",
    "#         self.random_state = random_state\n",
    "#         self.biases = np.random.uniform(size=(output_dim))\n",
    "#         self.loss_history = []\n",
    "    \n",
    "\n",
    "#     def set_weights(self, x):\n",
    "#         if self.weights is None:\n",
    "#             inp_dim = x.shape[1]\n",
    "#             self.weights = [np.random.uniform(low=-0.01, high=0.01, size=(inp_dim, self.neurons_per_layer[0]))]\n",
    "#             for i in range(1, self.n_hidden_layers):\n",
    "#                 self.weights.append(np.random.uniform(low=-0.01, high=0.01, size=(self.neurons_per_layer[i - 1], self.neurons_per_layer[i])))  # Initialize weights for subsequent hidden layers\n",
    "#         return self.weights\n",
    "    \n",
    "\n",
    "#     def forward_propagation(self, x):\n",
    "#         hidden_layers_output = [x]\n",
    "        \n",
    "#         for i in range(self.n_hidden_layers):\n",
    "#             dot_prod = np.dot(hidden_layers_output[-1], self.weights[i]) + self.biases\n",
    "#             output = sigmoid(dot_prod)\n",
    "#             hidden_layers_output.append(output)\n",
    "\n",
    "#         # final_output = hidden_layers_output[-1]\n",
    "#         final_output = sigmoid(dot_prod)\n",
    "#         return hidden_layers_output, final_output\n",
    "\n",
    "\n",
    "#     def backward_propagation(self, x, y, hidden_layers_output):\n",
    "#         del_weights = [None] * self.n_hidden_layers\n",
    "        \n",
    "#         # Calculate error\n",
    "#         error = y - hidden_layers_output[-1]\n",
    "\n",
    "#         # Calculate deltas for each layer\n",
    "#         del_output_error = error\n",
    "#         for i in reversed(range(self.n_hidden_layers)):\n",
    "#             del_dot_prod_error = der_sigmoid(hidden_layers_output[i + 1])\n",
    "#             del_weights[i] = np.dot(hidden_layers_output[i].T, del_output_error * del_dot_prod_error)\n",
    "#             print(del_output_error, self.weights[i].T.shape, del_dot_prod_error)\n",
    "#             del_output_error = np.dot(del_output_error, self.weights[i].T) * del_dot_prod_error\n",
    "\n",
    "#         return del_weights, del_output_error\n",
    "    \n",
    "\n",
    "#     def cross_entropy_loss(self, y, final_output):\n",
    "#         # Calculate cross-entropy loss\n",
    "#         epsilon = 1e-15  # Small value to prevent division by zero\n",
    "#         final_output = np.clip(final_output, epsilon, 1 - epsilon)\n",
    "#         loss = -np.mean(y * np.log(final_output) + (1 - y) * np.log(1 - final_output))\n",
    "#         return loss\n",
    "\n",
    "\n",
    "#     def train(self, x, y, epochs=30):\n",
    "#         np.random.seed(self.random_state)\n",
    "#         self.weights = self.set_weights(x)\n",
    "#         for _ in range(epochs):\n",
    "#             # Forward propagation\n",
    "#             hidden_layers_output, final_output = self.forward_propagation(x)\n",
    "            \n",
    "#             # Calculate error\n",
    "#             loss = self.cross_entropy_loss(y, final_output)\n",
    "#             self.loss_history.append(loss)\n",
    "\n",
    "#             # Backward propagation\n",
    "#             del_weights, del_output_error = self.backward_propagation(x, y, hidden_layers_output)\n",
    "            \n",
    "#             # Updating weights and bias\n",
    "#             for i in range(self.n_hidden_layers):\n",
    "#                 self.weights[i] += self.learning_rate * del_weights[i]\n",
    "#                 self.biases[i] += self.learning_rate * np.sum(del_output_error * der_sigmoid(hidden_layers_output[i + 1], axis=0))\n",
    "\n",
    "#             self.weights[-1] += self.learning_rate * np.dot(hidden_layers_output[-2].T, del_output_error * der_sigmoid(final_output))\n",
    "#             self.biases[-1] += self.learning_rate * np.sum(del_output_error * der_sigmoid(final_output), axis=0)\n",
    "          \n",
    "            \n",
    "#     def plot_loss(self):\n",
    "#         plt.plot(self.loss_history)\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Training Loss Over epochs')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was a failure, I'm trying with a new one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN2():\n",
    "    # Creating an object will initialize the network\n",
    "    def __init__(self, n_input_cols, n_hidden, n_output_cols):\n",
    "        self.network = list()\n",
    "        hidden_layer = [{'weights':[np.random.uniform(low=-0.01, high=0.01) for _ in range(n_input_cols + 1)]} for _ in range(n_hidden)]\n",
    "        self.network.append(hidden_layer)\n",
    "        output_layer = [{'weights':[np.random.uniform(low=-0.01, high=0.01) for _ in range(n_hidden + 1)]} for _ in range(n_output_cols)]\n",
    "        self.network.append(output_layer)\n",
    "\n",
    "\n",
    "    def forward_propagate(self, row):\n",
    "        def activate(weights, inputs):\n",
    "            activation = weights[-1]\n",
    "            for i in range(len(weights) - 1):\n",
    "                activation += weights[i] * inputs[i]\n",
    "            return activation\n",
    "\n",
    "        def neuron_transfer(activation):\n",
    "            return 1/(1 + np.exp(-activation))\n",
    "        \n",
    "        inputs = row\n",
    "        for layer in self.network:\n",
    "            new_input = []\n",
    "            for neuron in layer:\n",
    "                activation = activate(neuron['weights'], inputs)\n",
    "                neuron['output'] = neuron_transfer(activation)\n",
    "                new_input.append(neuron['output'])\n",
    "            inputs = new_input\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 13000\n",
    "n_features = 10\n",
    "n_classes = 2\n",
    "\n",
    "x_syn, y_syn = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative=6, n_redundant=4, random_state=42)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_syn, y_syn, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer:\n",
      "11 {'weights': [0.0009762700785464953, 0.0043037873274483895, 0.0020552675214328773, 0.0008976636599379376, -0.0015269040132219053, 0.002917882261333122, -0.0012482557747461494, 0.007835460015641596, 0.009273255210020587, -0.0023311696234844456, 0.00583450076165329]}\n",
      "11 {'weights': [0.000577898395058089, 0.0013608912218786469, 0.00851193276585322, -0.00857927883604226, -0.008257414005969186, -0.009595632051193485, 0.0066523969109587595, 0.005563135018997009, 0.007400242964936384, 0.00957236684465528, 0.005983171284334473]}\n",
      "11 {'weights': [-0.0007704127549413627, 0.00561058352572911, -0.007634511482621335, 0.0027984204265504766, -0.007132934251819072, 0.008893378340991678, 0.0004369664350014329, -0.0017067612001895275, -0.004708887757907461, 0.005484673788684334, -0.0008769933556690285]}\n",
      "11 {'weights': [0.0013686789773729707, -0.009624203991272897, 0.0023527099415175407, 0.002241914454448428, 0.0023386799374951386, 0.008874961570292482, 0.0036364059820696674, -0.00280984198852428, -0.0012593609240131708, 0.003952623918545298, -0.008795490567414604]}\n",
      "11 {'weights': [0.003335334308913354, 0.0034127573923631877, -0.005792348778523182, -0.007421474046902934, -0.0036914329815163228, -0.002725784581147548, 0.0014039354083575928, -0.0012279697307535937, 0.009767476761184524, -0.007959103785039438, -0.0058224648781033065]}\n",
      "11 {'weights': [-0.006773809642300075, 0.003062166509307969, -0.004934167949204358, -0.0006737845428738745, -0.005111488159967945, -0.006820608327089606, -0.007792497176713898, 0.003126591789305468, -0.0072363409730277235, -0.00606835276639893, -0.0026254965867807175]}\n",
      "11 {'weights': [0.006419864596958704, -0.008057974484138775, 0.006758898149976078, -0.008078031842120739, 0.009529189300267917, -0.0006269759670459677, 0.009535221763806742, 0.0020969103949009197, 0.004785271587966034, -0.009216244154913587, -0.004343860748471809]}\n",
      "11 {'weights': [-0.007596068775736622, -0.004077196049557101, -0.0076254456209151195, -0.0036403364121204793, -0.0017147401097066008, -0.008717050073024313, 0.0038494423874003963, 0.0013320290841315023, -0.004692210181211092, 0.00046496106933399325, -0.008121189784831166]}\n",
      "\n",
      "Output Layer:\n",
      "9 {'weights': [0.001518929911123585, 0.00858592395152428, -0.0036286209509735264, 0.003348207599273633, -0.007364042751912157, 0.004326544082371311, -0.004211878141055978, -0.006336172759857664, 0.0017302586962016624]}\n",
      "9 {'weights': [-0.00959784907625013, 0.0065788005843472625, -0.009906090476149059, 0.0035563307359246024, -0.004599840536156703, 0.004703880442451897, 0.009243770902348765, -0.00502493712960084, 0.0015231466883567375]}\n",
      "\n",
      "Output for row 0: [0.4999897635809492, 0.49976494956020445]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "ann = ANN2(x_train.shape[1], int(2/3 * x_train.shape[1]) + 2, 2) # Should be number of columns in input x, number of neurons per hidden layer (arbitrary, and we only have one hidden layer rn), and number of possible outputs (binary classification = 2)\n",
    "\n",
    "print(\"Hidden Layer:\")\n",
    "for neuron in ann.network[0]:\n",
    "    print(len(neuron['weights']), neuron)\n",
    "\n",
    "print(\"\\nOutput Layer:\")\n",
    "for neuron in ann.network[1]:\n",
    "    print(len(neuron['weights']), neuron)\n",
    "\n",
    "# Testing with a row\n",
    "row = x_train[0]\n",
    "output = ann.forward_propagate(row)\n",
    "print(f\"\\nOutput for row 0: {output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "dry_bean_dataset = fetch_ucirepo(id=602) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "x_uci = dry_bean_dataset.data.features \n",
    "y_uci = dry_bean_dataset.data.targets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_uci.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_uci[:1]) \n",
    "print(y_uci[:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
