{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML lab 1 \n",
    "- Assignment on numpy and pandas. Recap concepts covered in class. <br>\n",
    "- Use 2 datasets. One cleaned, one uncleaned. <br>\n",
    "- Exploring all possible numpy operations on a specific dataset (2 of them) or creating a list or array with clear comments as per your observation and application-level understanding. <br>\n",
    "- Apply 10-15 numpy (if numerical) functions or pandas (if categorical) functions on the datasets, cleanly document your code and ensure clarity for any reader. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysed the Datasets, these are the conclusions:\n",
    "- AmazonReviewAnalysis.csv - Primarily text data, originally to be used for Hierarchical Analysis (further study needed). Tasks: \n",
    "    - Loading the dataset, \n",
    "    - check for missing values, \n",
    "    - description function, \n",
    "    - Examine if I can eliminate rows with missing values and doing so, \n",
    "    - eliminating duplicate entries, \n",
    "    - normalization by converting all text to lower case for uniformity, \n",
    "    - remove unnecessary spacing in columns,\n",
    "    - convert cat1, cat2, cat3 to numerical data for easier processing during an ML task, \n",
    "    - plot numerical score and count for distribution understanding and outliers,\n",
    "    - plot review length and count distribution to understand review sizes,\n",
    "    - Length vs score analysis to see if longer reviews are more negative or positive\n",
    "    - Average score given by repetitive reviewers\n",
    "<br><br>\n",
    "- Cancer Patient Data Sets.csv - Primarily numerical data, clean, used for lung cancer analysis. Tasks:\n",
    "    - Average age to see which ages are at risk\n",
    "    - Average cancer levels\n",
    "    - Gender count to check which gender might be more at risk\n",
    "    - Since air pollution can heighten already existing genetic risks, we could explore this relationship\n",
    "    - Median chronic lung disease number\n",
    "    - Check for a relationship between obesity and cancer if any\n",
    "    - Identify range of alcohol consumption in cancer patients to figure out if higher-than-average alcohol increases lung cancer risks\n",
    "    - Standard deviation of weight loss to identify weight loss range in patients. This is because weight loss may be associated with lung cancer and allow early diagnosis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: AmazonReviewAnalysis.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports and initializations\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder as l\n",
    "le = l()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    productId                                              Title  \\\n",
      "0  B0002AQK70     PetSafe Staywell Pet Door with Clear Hard Flap   \n",
      "1  B0002DK8OI                      Kaytee Timothy Cubes, 1-Pound   \n",
      "2  B0006VJ6TO                                    Body Back Buddy   \n",
      "3  B000EZSFXA         SnackMasters California Style Turkey Jerky   \n",
      "4  B000KV61FC  Premier Busy Buddy Tug-a-Jug Treat Dispensing ...   \n",
      "\n",
      "           userId Helpfulness  Score        Time  \\\n",
      "0  A2L6QTQQI13LZG         1/1    4.0  1344211200   \n",
      "1   A2HJUOZ9R9K4F         0/0    1.0  1344211200   \n",
      "2  A14PK96LL78NN3         0/0    5.0  1344211200   \n",
      "3  A2UW73HU9UMOTY         0/0    5.0  1344211200   \n",
      "4  A1Q99RNV0TKW8R         1/1    4.0  1344211200   \n",
      "\n",
      "                                                Text                  Cat1  \\\n",
      "0  We've only had it installed about 2 weeks. So ...          pet supplies   \n",
      "1  My bunny had a hard time eating this because t...          pet supplies   \n",
      "2  would never in a million years have guessed th...  health personal care   \n",
      "3  Being the jerky fanatic I am, snackmasters han...  grocery gourmet food   \n",
      "4  Wondered how quick my dog would catch on to th...          pet supplies   \n",
      "\n",
      "                   Cat2                Cat3  \n",
      "0                  cats           cat flaps  \n",
      "1  bunny rabbit central                food  \n",
      "2           health care  massage relaxation  \n",
      "3            snack food   jerky dried meats  \n",
      "4                  dogs                toys  \n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load the dataset\n",
    "df = pd.read_csv(\"AmazonReviewAnalysis.csv\")\n",
    "\n",
    "# Comment: Data Frame successfully created from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has no null values.\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Check for Missing values\n",
    "if not any(df.isnull().sum()):\n",
    "    print(\"Has null values\")\n",
    "else:\n",
    "    print(\"Has no null values.\")\n",
    "\n",
    "# Comment: No null values to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Score          Time\n",
      "count  10000.000000  1.000000e+04\n",
      "mean       4.216700  1.355203e+09\n",
      "std        1.273068  4.566215e+06\n",
      "min        1.000000  1.344211e+09\n",
      "25%        4.000000  1.352592e+09\n",
      "50%        5.000000  1.356394e+09\n",
      "75%        5.000000  1.358640e+09\n",
      "max        5.000000  1.362269e+09\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Examine description of the dataset\n",
    "print(df.describe())\n",
    "\n",
    "# Comment: Values observed as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Eliminating any duplicate entries\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Comment: Any duplicate values are now removed. Only unique values will be worked with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Normalizing text for uniformity during later tasks, such as any language processing or text classification in future. \n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column] = df[column].str.lower()\n",
    "        df[column] = df[column].str.strip()\n",
    "\n",
    "# Comment: Successfully converted text to lowercase and removed unnecessary spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Converting categories to numerical data for later classification and analysis. \n",
    "attributes = ['Cat1', 'Cat2', 'Cat3']\n",
    "for i in attributes:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "\n",
    "# Comment: Data successfully converted so that categories are now numerical. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
